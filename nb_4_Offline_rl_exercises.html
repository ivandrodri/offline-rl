
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Offline RL algorithms exercises &#8212; Offline Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nb_4_Offline_rl_exercises';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Offline RL theory" href="nb_3_offline_RL_theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Offline Reinforcement Learning</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Offline RL Notes
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="nb_0_IntroOfflineRL.html">Introduction to Offline Reinforcement Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb_1_offline_RL_dataset_libraries.html">Open Source Datasets libraries for offline RL</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb_2_data_dollection.html">Exercise: Minari data collection</a></li>
<li class="toctree-l1"><a class="reference internal" href="nb_3_offline_RL_theory.html">Offline RL theory</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Offline RL algorithms exercises</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/nb_4_Offline_rl_exercises.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Offline RL algorithms exercises</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-i">Exercise I</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-create-the-environment">STEP 1: Create the environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-create-minari-datasets">STEP 2: Create Minari datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-feed-data-into-replay-buffer">STEP 3: Feed data into replay buffer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-analysis">Data analysis</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-5-select-offline-policies-and-training">STEP 4-5: Select offline policies and training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-conclusions">Summary and conclusions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-ii">Exercise II</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">STEP 1: Create the environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">STEP 2: Create Minari datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">STEP 3: Feed data into replay buffer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-select-offline-policies-and-training">STEP 4: Select offline policies and training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#restore-and-visualize-trained-policy"><strong>Restore and visualize trained policy</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%capture</span>
<span class="o">%</span><span class="n">load_ext</span> <span class="n">autoreload</span>
<span class="o">%</span><span class="n">autoreload</span> <span class="mi">2</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="o">%</span><span class="n">load_ext</span> <span class="n">offline_rl</span>
<span class="o">%</span><span class="n">set_random_seed</span> <span class="mi">12</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">presentation_style</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_latex_macros</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">autoreload</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">offline_rl.load_env_variables</span> <span class="kn">import</span> <span class="n">load_env_variables</span>

<span class="n">load_env_variables</span><span class="p">()</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register</span> <span class="kn">import</span> <span class="n">ObstacleTypes</span>
<span class="kn">from</span> <span class="nn">offline_rl.custom_envs.custom_envs_registration</span> <span class="kn">import</span> <span class="n">CustomEnv</span><span class="p">,</span> <span class="n">RenderMode</span><span class="p">,</span> <span class="n">EnvFactory</span>
<span class="kn">from</span> <span class="nn">offline_rl.offline_policies.offpolicy_rendering</span> <span class="kn">import</span> <span class="n">offpolicy_rendering</span>
<span class="kn">from</span> <span class="nn">offline_rl.offline_trainings.policy_config_data_class</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">TrainedPolicyConfig</span><span class="p">,</span>
    <span class="n">get_trained_policy_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">offline_rl.offline_trainings.offline_training</span> <span class="kn">import</span> <span class="n">OfflineRLTraining</span>
<span class="kn">from</span> <span class="nn">offline_rl.utils</span> <span class="kn">import</span> <span class="n">widget_list</span>
<span class="kn">from</span> <span class="nn">offline_rl.visualizations.utils</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">get_state_action_data_and_policy_grid_distributions</span><span class="p">,</span>
    <span class="n">snapshot_env</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">offline_rl.utils</span> <span class="kn">import</span> <span class="n">load_buffer_minari</span>
<span class="kn">from</span> <span class="nn">offline_rl.custom_envs.env_wrappers</span> <span class="kn">import</span> <span class="n">Grid2DInitialConfig</span>
<span class="kn">from</span> <span class="nn">offline_rl.behavior_policies.behavior_policy_registry</span> <span class="kn">import</span> <span class="n">BehaviorPolicy2dGridFactory</span>
<span class="kn">from</span> <span class="nn">offline_rl.offline_trainings.training_interface</span> <span class="kn">import</span> <span class="n">OfflineTrainingHyperparams</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>


<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">render_mode</span> <span class="o">=</span> <span class="n">RenderMode</span><span class="o">.</span><span class="n">RGB_ARRAY</span>
</pre></div>
</div>
</div>
</div>
<p>Offline RL pipeline:</p>
<img src="_static/images/93_offline_RL_pipeline.png" alt="Snow" style="width:50%;"><section class="tex2jax_ignore mathjax_ignore" id="offline-rl-algorithms-exercises">
<h1>Offline RL algorithms exercises<a class="headerlink" href="#offline-rl-algorithms-exercises" title="Link to this heading">#</a></h1>
<p><strong>Off-policy methods cannot learn from data efficiently unless a significant amount of data covering a large portion of the environment states is available</strong>. Only in such cases can the agent explore the environment and get feedback similar to what’s done in an online approach. However, this scenario is rare and challenging to achieve in realistic applications, which is one of the reasons why we turn to offline RL, where only a small amount of data is available.</p>
<p>One of the major issues when applying off-policy methods to collected data is the agent’s tendency to go out-of-distribution (o.o.d.). More importantly, once it goes o.o.d., the policy becomes unpredictable, making it impossible to return to the in-distribution region. This unpredictability propagates errors in the policy evaluation process (i.e., the dynamic programming equations), destroying the algorithm’s learning capabilities.</p>
<section id="exercise-i">
<h2>Exercise I<a class="headerlink" href="#exercise-i" title="Link to this heading">#</a></h2>
<p>Let’s collect a small amount of expert data and a larger amount of suboptimal data. We will play with two offline RL algorithms, BCQ and CQL, and we will check it they can recover the expert policy without going o.o.d. We will compare the results with the imitation learning approach, specifically the BC algorithm, which in some cases is another viable option when expert data is available.</p>
<p>In this exercise, we will collect two datasets: one with expert and another with suboptimal data. The goal of the agent will be to get as close as possible to the target.</p>
<p>I - <strong>expert policy</strong>: collect ~ 1000 steps</p>
<p>II  - <strong>Suboptimal policy</strong>:  collect ~ 2000 steps</p>
<section id="step-1-create-the-environment">
<h3>STEP 1: Create the environment<a class="headerlink" href="#step-1-create-the-environment" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">obstacle_selected</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">([</span><span class="n">ObstacleTypes</span><span class="o">.</span><span class="n">obstacle_8x8_wall_with_door</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ENV</span> <span class="o">=</span> <span class="n">EnvFactory</span><span class="o">.</span><span class="n">Grid_2D_8x8_discrete</span>

<span class="c1"># Grid configuration</span>
<span class="n">OBSTACLE</span> <span class="o">=</span> <span class="n">obstacle_selected</span><span class="o">.</span><span class="n">value</span>
<span class="n">INITIAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">FINAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">env_2D_grid_initial_config</span> <span class="o">=</span> <span class="n">Grid2DInitialConfig</span><span class="p">(</span>
    <span class="n">obstacles</span><span class="o">=</span><span class="n">OBSTACLE</span><span class="p">,</span>
    <span class="n">initial_state</span><span class="o">=</span><span class="n">INITIAL_STATE</span><span class="p">,</span>
    <span class="n">target_state</span><span class="o">=</span><span class="n">FINAL_STATE</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">ENV</span><span class="o">.</span><span class="n">get_env</span><span class="p">(</span><span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span> <span class="n">grid_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">)</span>

<span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-create-minari-datasets">
<h3>STEP 2: Create Minari datasets<a class="headerlink" href="#step-2-create-minari-datasets" title="Link to this heading">#</a></h3>
<p><strong>Behavior policies and datasets configurations</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">BEHAVIOR_POLICY_I</span> <span class="o">=</span> <span class="n">BehaviorPolicy2dGridFactory</span><span class="o">.</span><span class="n">move_up</span>
<span class="n">DATA_SET_IDENTIFIER_I</span> <span class="o">=</span> <span class="s2">&quot;_expert&quot;</span>
<span class="n">NUM_STEPS_I</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">BEHAVIOR_POLICY_II</span> <span class="o">=</span> <span class="n">BehaviorPolicy2dGridFactory</span><span class="o">.</span><span class="n">move_left</span>
<span class="n">DATA_SET_IDENTIFIER_II</span> <span class="o">=</span> <span class="s2">&quot;_suboptimal&quot;</span>
<span class="n">NUM_STEPS_II</span> <span class="o">=</span> <span class="mi">2000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy_selected</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">([</span><span class="n">BEHAVIOR_POLICY_I</span><span class="p">,</span> <span class="n">BEHAVIOR_POLICY_II</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">offpolicy_rendering</span><span class="p">(</span>
    <span class="n">env_or_env_name</span><span class="o">=</span><span class="n">ENV</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">behavior_policy</span><span class="o">=</span><span class="n">policy_selected</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">,</span>
    <span class="n">num_frames</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Collect data</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">offline_rl.generate_custom_minari_datasets.generate_minari_dataset</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">create_combined_minari_dataset</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">config_combined_data</span> <span class="o">=</span> <span class="n">create_combined_minari_dataset</span><span class="p">(</span>
    <span class="n">env_name</span><span class="o">=</span><span class="n">ENV</span><span class="p">,</span>
    <span class="n">dataset_identifiers</span><span class="o">=</span><span class="p">(</span><span class="n">DATA_SET_IDENTIFIER_I</span><span class="p">,</span> <span class="n">DATA_SET_IDENTIFIER_II</span><span class="p">),</span>
    <span class="n">num_collected_points</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_STEPS_I</span><span class="p">,</span> <span class="n">NUM_STEPS_II</span><span class="p">),</span>
    <span class="n">behavior_policies</span><span class="o">=</span><span class="p">(</span><span class="n">BEHAVIOR_POLICY_I</span><span class="p">,</span> <span class="n">BEHAVIOR_POLICY_II</span><span class="p">),</span>
    <span class="n">combined_dataset_identifier</span><span class="o">=</span><span class="s2">&quot;combined_data_sets_offline_rl&quot;</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset_availables</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">config_combined_data</span><span class="o">.</span><span class="n">data_set_name</span>
<span class="p">]</span> <span class="o">+</span> <span class="n">config_combined_data</span><span class="o">.</span><span class="n">children_dataset_names</span>
<span class="n">selected_data_set</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">(</span><span class="n">dataset_availables</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-feed-data-into-replay-buffer">
<h3>STEP 3: Feed data into replay buffer<a class="headerlink" href="#step-3-feed-data-into-replay-buffer" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">buffer_data</span> <span class="o">=</span> <span class="n">load_buffer_minari</span><span class="p">(</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="n">len_buffer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">)</span>

<span class="c1"># Compute state-action data distribution</span>
<span class="n">state_action_count_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_state_action_data_and_policy_grid_distributions</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">,</span> <span class="n">env</span><span class="p">)</span>
<span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="data-analysis">
<h4>Data analysis<a class="headerlink" href="#data-analysis" title="Link to this heading">#</a></h4>
<p>Note that we have four peaks. The ones at (2,7) and (3,7) come from policy-I, which goes towards the target but stops before reaching it. The other two peaks at (6,0) and (7,0) are produced by policy-II, which drifts the agent to the left with noise. <strong>It is important to notice that the amount of collected data at state (5,7) is very little, but this state is crucial if we want to approach the target.</strong></p>
<p>What do you think a BC algorithm would do? What about an offline one?</p>
<div style="margin-top: 20px;">
    <div style="display: flex; justify-content: space-between;">
        <div style="width: 100%;">
            <img src="_static/images/nb_96_critical_state.png" alt="Snow" style="width:100%;">
        </div>
        <div style="width: 100%;">
            <img src="_static/images/96_critical_action_states.png" alt="KL divergence" width=80%>
        </div>
    </div>
</div></section>
</section>
<section id="step-4-5-select-offline-policies-and-training">
<h3>STEP 4-5: Select offline policies and training<a class="headerlink" href="#step-4-5-select-offline-policies-and-training" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">offline_rl.offline_policies.policy_registry</span> <span class="kn">import</span> <span class="n">RLPolicyFactory</span>

<span class="n">offline_rl_policies</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">RLPolicyFactory</span><span class="o">.</span><span class="n">bcq_discrete</span><span class="p">,</span>
    <span class="n">RLPolicyFactory</span><span class="o">.</span><span class="n">cql_discrete</span><span class="p">,</span>
    <span class="n">RLPolicyFactory</span><span class="o">.</span><span class="n">imitation_learning</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">selected_offline_rl_policy</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">(</span><span class="n">offline_rl_policies</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Training</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">STEP_PER_EPOCH</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">len_buffer</span>
<span class="n">NUMBER_TEST_ENVS</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Metadata for the offline policy. Included the Minari metadata as well as the policy model configuration.</span>
<span class="n">offline_policy_config</span> <span class="o">=</span> <span class="n">TrainedPolicyConfig</span><span class="p">(</span>
    <span class="n">rl_policy_model</span><span class="o">=</span><span class="n">selected_offline_rl_policy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">name_expert_data</span><span class="o">=</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Hyperparameters to be used in the training.</span>
<span class="n">offline_training_hyperparams</span> <span class="o">=</span> <span class="n">OfflineTrainingHyperparams</span><span class="p">(</span>
    <span class="n">offline_policy_config</span><span class="o">=</span><span class="n">offline_policy_config</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">,</span>
    <span class="n">number_test_envs</span><span class="o">=</span><span class="n">NUMBER_TEST_ENVS</span><span class="p">,</span>
    <span class="n">step_per_epoch</span><span class="o">=</span><span class="n">STEP_PER_EPOCH</span><span class="p">,</span>
    <span class="n">restore_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OfflineRLTraining</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">offline_training_hyperparams</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Restore and visualize trained policy</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">available_obstacles</span> <span class="o">=</span> <span class="p">[</span><span class="n">ObstacleTypes</span><span class="o">.</span><span class="n">obstacle_8x8_wall_with_door</span><span class="p">]</span>
<span class="n">selected_obstacle</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">(</span><span class="n">available_obstacles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SAVED_POLICY_NAME = &quot;policy_best_reward.pth&quot;</span>
<span class="n">SAVED_POLICY_NAME</span> <span class="o">=</span> <span class="s2">&quot;policy.pth&quot;</span>
<span class="n">INITIAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">FINAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>

<span class="n">offline_policy_config</span> <span class="o">=</span> <span class="n">TrainedPolicyConfig</span><span class="p">(</span>
    <span class="n">name_expert_data</span><span class="o">=</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">rl_policy_model</span><span class="o">=</span><span class="n">selected_offline_rl_policy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">OfflineRLTraining</span><span class="o">.</span><span class="n">restore_policy</span><span class="p">(</span><span class="n">offline_policy_config</span><span class="p">)</span>

<span class="n">env</span><span class="o">.</span><span class="n">set_new_obstacle_map</span><span class="p">(</span><span class="n">selected_obstacle</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_starting_point</span><span class="p">(</span><span class="n">INITIAL_STATE</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_goal_point</span><span class="p">(</span><span class="n">FINAL_STATE</span><span class="p">)</span>
<span class="c1"># snapshot_env(env)</span>

<span class="n">offpolicy_rendering</span><span class="p">(</span>
    <span class="n">env_or_env_name</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">policy_model</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">,</span>
    <span class="n">num_frames</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">imitation_policy_sampling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">inline</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary-and-conclusions">
<h3>Summary and conclusions<a class="headerlink" href="#summary-and-conclusions" title="Link to this heading">#</a></h3>
<p><strong>BCQ and the CQL policies are able to learn the expert data</strong></p>
<p><strong>Imitation learning cannot make it because it cannot learn the critical state-action pairs.</strong></p>
</section>
</section>
<section id="exercise-ii">
<h2>Exercise II<a class="headerlink" href="#exercise-ii" title="Link to this heading">#</a></h2>
<p><strong>Now, we’ll explore how BCQ and CQL, address the issue of connecting suboptimal trajectories in order to get new ones with higer rewards (stitching property). We will see how they compare with imitation learning.</strong></p>
<p>We will start again with the previous setup. So, as we did before, we will create again two datasets: one from a policy moving suboptimal from (0,0) to (2,4), and the other from another policy moving from (4,0) to (7,7). The goal is to find an agent capable of connecting trajectories coming from both datasets, in order to find the optimal path between (2,0) and (2,4).</p>
<section id="id1">
<h3>STEP 1: Create the environment<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p><strong>Create the environment</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ENV</span> <span class="o">=</span> <span class="n">EnvFactory</span><span class="o">.</span><span class="n">Grid_2D_8x8_discrete</span>

<span class="n">OBSTACLE</span> <span class="o">=</span> <span class="n">ObstacleTypes</span><span class="o">.</span><span class="n">obst_free_8x8</span>
<span class="n">INITIAL_STATE_POLICY_I</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">INITIAL_STATE_POLICY_II</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">FINAL_STATE_POLICY</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>


<span class="n">env_2D_grid_initial_config_I</span> <span class="o">=</span> <span class="n">Grid2DInitialConfig</span><span class="p">(</span>
    <span class="n">obstacles</span><span class="o">=</span><span class="n">OBSTACLE</span><span class="p">,</span>
    <span class="n">initial_state</span><span class="o">=</span><span class="n">INITIAL_STATE_POLICY_I</span><span class="p">,</span>
    <span class="n">target_state</span><span class="o">=</span><span class="n">FINAL_STATE_POLICY</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">env_2D_grid_initial_config_II</span> <span class="o">=</span> <span class="n">Grid2DInitialConfig</span><span class="p">(</span>
    <span class="n">obstacles</span><span class="o">=</span><span class="n">OBSTACLE</span><span class="p">,</span>
    <span class="n">initial_state</span><span class="o">=</span><span class="n">INITIAL_STATE_POLICY_II</span><span class="p">,</span>
    <span class="n">target_state</span><span class="o">=</span><span class="n">FINAL_STATE_POLICY</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">ENV</span><span class="o">.</span><span class="n">get_env</span><span class="p">(</span><span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span> <span class="n">grid_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config_I</span><span class="p">)</span>
<span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">ENV</span><span class="o">.</span><span class="n">get_env</span><span class="p">(</span><span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span> <span class="n">grid_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config_II</span><span class="p">)</span>
<span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3>STEP 2: Create Minari datasets<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p><strong>Let’s see how well offline RL algorithms can deal with the stitching property. We will examine some edge cases to compare them with some of the algorithms we have already studied before.</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">IDENTIFIER_COMBINED_DATASETS</span> <span class="o">=</span> <span class="s2">&quot;_stiching_property_I&quot;</span>

<span class="c1"># Dataset I with 2000 collected points</span>
<span class="n">BEHAVIOR_POLICY_I</span> <span class="o">=</span> <span class="n">BehaviorPolicy2dGridFactory</span><span class="o">.</span><span class="n">deterministic_8x8</span>
<span class="n">DATA_SET_IDENTIFIER_I</span> <span class="o">=</span> <span class="s2">&quot;_longer_path&quot;</span>
<span class="n">NUM_STEPS_I</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="c1"># Dataset II with 1000 points</span>
<span class="c1"># BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7</span>
<span class="n">BEHAVIOR_POLICY_II</span> <span class="o">=</span> <span class="n">BehaviorPolicy2dGridFactory</span><span class="o">.</span><span class="n">move_right</span>
<span class="n">DATA_SET_IDENTIFIER_II</span> <span class="o">=</span> <span class="s2">&quot;_short_path&quot;</span>
<span class="n">NUM_STEPS_II</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">select_policy_to_render</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">([</span><span class="n">BEHAVIOR_POLICY_I</span><span class="p">,</span> <span class="n">BEHAVIOR_POLICY_II</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env_2D_grid_initial_config</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">env_2D_grid_initial_config_I</span>
    <span class="k">if</span> <span class="n">select_policy_to_render</span><span class="o">.</span><span class="n">value</span> <span class="o">==</span> <span class="n">BEHAVIOR_POLICY_I</span>
    <span class="k">else</span> <span class="n">env_2D_grid_initial_config_II</span>
<span class="p">)</span>

<span class="n">offpolicy_rendering</span><span class="p">(</span>
    <span class="n">env_or_env_name</span><span class="o">=</span><span class="n">ENV</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">behavior_policy_name</span><span class="o">=</span><span class="n">select_policy_to_render</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">,</span>
    <span class="n">num_frames</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Create datasets</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">config_combined_data</span> <span class="o">=</span> <span class="n">create_combined_minari_dataset</span><span class="p">(</span>
    <span class="n">env_name</span><span class="o">=</span><span class="n">ENV</span><span class="p">,</span>
    <span class="n">dataset_identifiers</span><span class="o">=</span><span class="p">(</span><span class="n">DATA_SET_IDENTIFIER_I</span><span class="p">,</span> <span class="n">DATA_SET_IDENTIFIER_II</span><span class="p">),</span>
    <span class="n">num_collected_points</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_STEPS_I</span><span class="p">,</span> <span class="n">NUM_STEPS_II</span><span class="p">),</span>
    <span class="n">behavior_policies</span><span class="o">=</span><span class="p">(</span><span class="n">BEHAVIOR_POLICY_I</span><span class="p">,</span> <span class="n">BEHAVIOR_POLICY_II</span><span class="p">),</span>
    <span class="n">combined_dataset_identifier</span><span class="o">=</span><span class="s2">&quot;combined_dataset&quot;</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="p">(</span><span class="n">env_2D_grid_initial_config_I</span><span class="p">,</span> <span class="n">env_2D_grid_initial_config_II</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">buffer_data</span> <span class="o">=</span> <span class="n">load_buffer_minari</span><span class="p">(</span><span class="n">config_combined_data</span><span class="o">.</span><span class="n">data_set_name</span><span class="p">)</span>
<span class="n">data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_availables</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">config_combined_data</span><span class="o">.</span><span class="n">data_set_name</span>
<span class="p">]</span> <span class="o">+</span> <span class="n">config_combined_data</span><span class="o">.</span><span class="n">children_dataset_names</span>
<span class="n">selected_data_set</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">(</span><span class="n">dataset_availables</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3>STEP 3: Feed data into replay buffer<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">buffer_data</span> <span class="o">=</span> <span class="n">load_buffer_minari</span><span class="p">(</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="n">len_buffer</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer_data</span><span class="p">)</span>

<span class="c1"># Compute state-action data distribution</span>
<span class="n">state_action_count_data</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_state_action_data_and_policy_grid_distributions</span><span class="p">(</span>
    <span class="n">buffer_data</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="k">if</span> <span class="s2">&quot;start_0_0&quot;</span> <span class="ow">in</span> <span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
    <span class="n">env</span><span class="o">.</span><span class="n">set_starting_point</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
<span class="k">elif</span> <span class="s2">&quot;start_2_0&quot;</span> <span class="ow">in</span> <span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
    <span class="n">env</span><span class="o">.</span><span class="n">set_starting_point</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">snapshot_env</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-select-offline-policies-and-training">
<h3>STEP 4: Select offline policies and training<a class="headerlink" href="#step-4-select-offline-policies-and-training" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">offline_rl_policies</span> <span class="o">=</span> <span class="p">[</span><span class="n">RLPolicyFactory</span><span class="o">.</span><span class="n">bcq_discrete</span><span class="p">,</span> <span class="n">RLPolicyFactory</span><span class="o">.</span><span class="n">cql_discrete</span><span class="p">]</span>
<span class="n">selected_offline_rl_policy</span> <span class="o">=</span> <span class="n">widget_list</span><span class="p">(</span><span class="n">offline_rl_policies</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Offiline - Training</span>

<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">STEP_PER_EPOCH</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="n">len_buffer</span>
<span class="n">NUMBER_TEST_ENVS</span> <span class="o">=</span> <span class="mi">1</span>


<span class="n">offline_policy_config</span> <span class="o">=</span> <span class="n">TrainedPolicyConfig</span><span class="p">(</span>
    <span class="n">name_expert_data</span><span class="o">=</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">rl_policy_model</span><span class="o">=</span><span class="n">selected_offline_rl_policy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span>


<span class="c1"># Hyperparameters to be used in the training.</span>
<span class="n">offline_training_hyperparams</span> <span class="o">=</span> <span class="n">OfflineTrainingHyperparams</span><span class="p">(</span>
    <span class="n">offline_policy_config</span><span class="o">=</span><span class="n">offline_policy_config</span><span class="p">,</span>
    <span class="n">num_epochs</span><span class="o">=</span><span class="n">NUM_EPOCHS</span><span class="p">,</span>
    <span class="n">number_test_envs</span><span class="o">=</span><span class="n">NUMBER_TEST_ENVS</span><span class="p">,</span>
    <span class="n">step_per_epoch</span><span class="o">=</span><span class="n">STEP_PER_EPOCH</span><span class="p">,</span>
    <span class="n">restore_training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OfflineRLTraining</span><span class="o">.</span><span class="n">training</span><span class="p">(</span><span class="n">offline_training_hyperparams</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="restore-and-visualize-trained-policy">
<h4><strong>Restore and visualize trained policy</strong><a class="headerlink" href="#restore-and-visualize-trained-policy" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># SAVED_POLICY_NAME = &quot;policy_best_reward.pth&quot;</span>
<span class="n">SAVED_POLICY_NAME</span> <span class="o">=</span> <span class="s2">&quot;policy.pth&quot;</span>
<span class="n">INITIAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">FINAL_STATE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">offline_policy_config</span> <span class="o">=</span> <span class="n">TrainedPolicyConfig</span><span class="p">(</span>
    <span class="n">name_expert_data</span><span class="o">=</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">rl_policy_model</span><span class="o">=</span><span class="n">selected_offline_rl_policy</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">OfflineRLTraining</span><span class="o">.</span><span class="n">restore_policy</span><span class="p">(</span><span class="n">offline_policy_config</span><span class="p">)</span>
<span class="n">log_name</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">selected_data_set</span><span class="o">.</span><span class="n">value</span><span class="p">)</span> <span class="o">/</span> <span class="n">Path</span><span class="p">(</span><span class="n">selected_offline_rl_policy</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="n">log_path</span> <span class="o">=</span> <span class="n">get_trained_policy_path</span><span class="p">()</span> <span class="o">/</span> <span class="n">log_name</span> <span class="o">/</span> <span class="n">SAVED_POLICY_NAME</span>
<span class="n">policy</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">log_path</span><span class="p">),</span> <span class="n">map_location</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">))</span>

<span class="n">env</span><span class="o">.</span><span class="n">set_starting_point</span><span class="p">(</span><span class="n">INITIAL_STATE</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">set_goal_point</span><span class="p">(</span><span class="n">FINAL_STATE</span><span class="p">)</span>
<span class="c1"># snapshot_env(env)</span>

<span class="n">offpolicy_rendering</span><span class="p">(</span>
    <span class="n">env_or_env_name</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
    <span class="n">render_mode</span><span class="o">=</span><span class="n">render_mode</span><span class="p">,</span>
    <span class="n">policy_model</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
    <span class="n">env_2d_grid_initial_config</span><span class="o">=</span><span class="n">env_2D_grid_initial_config</span><span class="p">,</span>
    <span class="n">num_frames</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">imitation_policy_sampling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Question</strong>: Let’s now change the dataset distribution. We’ll collect 600 points with the first behavior policy and 100 with the second one. In this case, the probability of taking the suboptimal path will be higher. What paths are chosen by the algorithms?</p>
</section>
</section>
</section>
<section id="final-remarks">
<h2>Final remarks<a class="headerlink" href="#final-remarks" title="Link to this heading">#</a></h2>
<p>Offline RL proves valuable in various scenarios, especially when:</p>
<p>a. Robots require intelligent behavior in complex open-world environments demanding extensive training data due to robust visual perception requirements. (complex environment modeling and extensive data collection)</p>
<p>b. Robot grasping tasks, which involve expert data that cannot be accurately simulated, providing an opportunity to assess our BCQ algorithm.</p>
<p>c. Robotic navigation tasks, where offline RL aids in crafting effective navigation policies using real-world data.</p>
<p>d. Autonomous driving, where ample expert data and an offline approach enhance safety.</p>
<p>e. Healthcare applications, where safety is paramount due to the potential serious consequences of inaccurate forecasts.</p>
<p>… and many more.</p>
<p>However, if you have access to an environment with abundant data, online Reinforcement Learning (RL) can be a powerful choice due to its potential for exploration and real-time feedback. Nevertheless, the landscape of RL is evolving, and a data-centric approach is gaining prominence, exemplified by vast datasets like X-Embodiment. It’s becoming evident that robots trained with diverse data across various scenarios tend to outperform those solely focused on specific tasks. Furthermore, leveraging multitask trained agents for transfer learning can be a valuable strategy for addressing your specific task at hand.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "offline-rl-example"
        },
        kernelOptions: {
            name: "offline-rl-example",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'offline-rl-example'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="nb_3_offline_RL_theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Offline RL theory</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-i">Exercise I</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-create-the-environment">STEP 1: Create the environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-create-minari-datasets">STEP 2: Create Minari datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-feed-data-into-replay-buffer">STEP 3: Feed data into replay buffer</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-analysis">Data analysis</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-5-select-offline-policies-and-training">STEP 4-5: Select offline policies and training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-and-conclusions">Summary and conclusions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-ii">Exercise II</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">STEP 1: Create the environment</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">STEP 2: Create Minari datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">STEP 3: Feed data into replay buffer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-select-offline-policies-and-training">STEP 4: Select offline policies and training</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#restore-and-visualize-trained-policy"><strong>Restore and visualize trained policy</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-remarks">Final remarks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ivan Rodriguez
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>