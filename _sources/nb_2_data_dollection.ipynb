{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir ../offline_rl/ --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from offline_rl.set_env_variables import set_env_variables\n",
    "\n",
    "set_env_variables()\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "import gymnasium as gym\n",
    "from offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from offline_rl.custom_envs.custom_2d_grid_env.simple_grid import Custom2DGridEnv\n",
    "from offline_rl.custom_envs.custom_envs_registration import RenderMode\n",
    "from offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from offline_rl.utils import load_buffer_minari\n",
    "from offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions,\n",
    "    snapshot_env,\n",
    ")\n",
    "from offline_rl.utils import widget_list\n",
    "from offline_rl.custom_envs.custom_envs_registration import EnvFactory\n",
    "from offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicy2dGridFactory\n",
    "from offline_rl.generate_custom_minari_datasets.generate_minari_dataset import (\n",
    "    create_minari_datasets,\n",
    ")\n",
    "\n",
    "\n",
    "if not os.environ.get(\"DISPLAY\"):\n",
    "    os.environ[\"MUJOCO_GL\"] = \"egl\"\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# To get access to the registered environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: Minari data collection\n",
    "\n",
    "In this exercise you don't have any homework. The idea is to play around with it to get familiar with the code  (notebooks and source code) and with the way we collect data.\n",
    "\n",
    "Remember that the pipeline for offline learning will be the following:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">\n",
    "\n",
    "In this notebook we will be exploring the steps 1-2-3 . It would be a good idea now to give a look to the code structure:\n",
    "\n",
    "<img src=\"_static/images/93_code_structure.png\" alt=\"Snow\" style=\"width:20%;\">\n",
    "\n",
    "You can also give a look to [Minari documentation](https://minari.farama.org/main/content/basic_usage/) if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STEP 1: Create the environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different environments\n",
    "ENV_LIST = [\n",
    "    EnvFactory.Grid_2D_8x8_discrete,\n",
    "    \"InvertedPendulum-v4\",\n",
    "    \"Humanoid-v4\",\n",
    "    \"AdroitHandHammer-v1\",\n",
    "    \"HalfCheetah-v4\",\n",
    "]\n",
    "\n",
    "# obstacles to be used with the 2d grid-world\n",
    "grid_world_obstacles = [\n",
    "    ObstacleTypes.obstacle_8x8_top_right,\n",
    "    ObstacleTypes.obst_free_8x8,\n",
    "]\n",
    "\n",
    "# behavior policies to be used with 2d grid-world\n",
    "behavior_policy_grid_world = [\n",
    "    BehaviorPolicy2dGridFactory.suboptimal_8x8,\n",
    "    BehaviorPolicy2dGridFactory.random,\n",
    "    BehaviorPolicy2dGridFactory.deterministic_8x8,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_environment = widget_list(ENV_LIST, description=\"Mixed envs.\")\n",
    "selected_obstacle = widget_list(grid_world_obstacles, description=\"grid obst.\")\n",
    "selected_grid_world_policy = widget_list(behavior_policy_grid_world, description=\"grid policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Select and render behavior policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_policy = BehaviorPolicy2dGridFactory.random\n",
    "if isinstance(selected_environment.value, EnvFactory):\n",
    "    env = EnvFactory[selected_environment.value].get_env(render_mode=RenderMode.RGB_ARRAY)\n",
    "    if isinstance(env.unwrapped, Custom2DGridEnv):\n",
    "        env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "        behavior_policy = selected_grid_world_policy.value\n",
    "else:\n",
    "    env = gym.make(selected_environment.value, render_mode=\"rgb_array\")\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=RenderMode.RGB_ARRAY,\n",
    "    behavior_policy=behavior_policy,\n",
    "    num_frames=100,\n",
    "    fps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_IDENTIFIER_I = \"_collected_data_nb_92\"\n",
    "NUM_STEPS_I = 500\n",
    "\n",
    "data_set_config = create_minari_datasets(\n",
    "    env_name=env.spec.id,\n",
    "    dataset_identifier=DATA_SET_IDENTIFIER_I,\n",
    "    num_collected_points=NUM_STEPS_I,\n",
    "    behavior_policy=behavior_policy,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations! You have created your first Minari dataset. Take a look at `src/offline_rl/data`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## STEP 3: Feed dataset to Tianshou ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(data_set_config.data_set_name)\n",
    "print(f\"The number of dataset points is {len(buffer_data)}\")\n",
    "\n",
    "for elem in buffer_data:\n",
    "    print(elem)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's give a look to the collected 2d grid world data**\n",
    "\n",
    "**Useful information:**\n",
    "\n",
    "In our grid world environment, the agent's position is represented as $(x_1, x_2)$, with $x_1/x_2$ the vertical/horizontal coordinates. Observations are represented as a one-hot encoded vector of dimensions $x_1 \\times x_2$, for example, a 64-dimensional vector in an 8x8 grid,\n",
    " \n",
    "\n",
    "The action is represented by an integer in the range of [0, 1, 2, 3], each indicating a direction:\n",
    "\n",
    "    0: (-1, 0) - UP\n",
    "    1: (1, 0) - DOWN\n",
    "    2: (0, -1) - LEFT\n",
    "    3: (0, 1) - RIGHT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute state-action data distribution\n",
    "if type(env.unwrapped).__name__== \"Custom2DGridEnv\":\n",
    "    state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(\n",
    "        buffer_data, env\n",
    "    )\n",
    "    snapshot_env(env)\n",
    "else:\n",
    "    raise ValueError(f\"To analyze the data the environment should be of type {Custom2DGridEnv}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "[ \\[Fu.Justin et. al.\\] D4RL: Datasets for Deep Data-Driven Reinforcement Learning](https://arxiv.org/abs/2004.07219)\n",
    "\n",
    "[ MINARI: A dataset API for Offline Reinforcement Learning ](https://minari.farama.org/main/content/basic_usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
