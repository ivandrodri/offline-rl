{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%load_ext offline_rl\n",
    "%set_random_seed 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from offline_rl.set_env_variables import set_env_variables\n",
    "set_env_variables()\n",
    "import torch\n",
    "import warnings\n",
    "from offline_rl.custom_envs.custom_2d_grid_env.obstacles_2D_grid_register import ObstacleTypes\n",
    "from offline_rl.custom_envs.custom_envs_registration import RenderMode, EnvFactory\n",
    "from offline_rl.offline_policies.offpolicy_rendering import offpolicy_rendering\n",
    "from offline_rl.trainings.policy_config_data_class import (\n",
    "    TrainedPolicyConfig,\n",
    "    get_trained_policy_path,\n",
    ")\n",
    "from offline_rl.trainings.offline_training import OfflineRLTraining\n",
    "from offline_rl.utils import widget_list\n",
    "from offline_rl.visualizations.utils import (\n",
    "    get_state_action_data_and_policy_grid_distributions,\n",
    "    snapshot_env,\n",
    ")\n",
    "from offline_rl.utils import load_buffer_minari\n",
    "from offline_rl.custom_envs.env_wrappers import Grid2DInitialConfig\n",
    "from offline_rl.behavior_policies.behavior_policy_registry import BehaviorPolicy2dGridFactory\n",
    "from offline_rl.trainings.training_interface import OfflineTrainingHyperparams\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "render_mode = RenderMode.RGB_ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL pipeline:\n",
    "\n",
    "<img src=\"_static/images/93_offline_RL_pipeline.png\" alt=\"Snow\" style=\"width:50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise: Offline RL algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Off-policy methods cannot learn from data efficiently unless a significant amount of data covering a large portion of the environment states is available**. Only in such cases can the agent explore the environment and get feedback similar to what's done in an online approach. However, this scenario is rare and challenging to achieve in realistic applications, which is one of the reasons why we turn to offline RL, where only a small amount of data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One of the major issues when applying off-policy methods to collected data is the agent's tendency to go out-of-distribution (o.o.d.). More importantly, once it goes o.o.d., the policy becomes unpredictable, making it impossible to return to the in-distribution region. This unpredictability propagates errors in the policy evaluation process (i.e., the dynamic programming equations), destroying the algorithm's learning capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise I\n",
    "\n",
    "Let's collect a small amount of expert data and a larger amount of suboptimal data. We will play with two offline RL algorithms, BCQ and CQL, and we will check it they can recover the expert policy without going o.o.d. We will compare the results with the imitation learning approach, specifically the BC algorithm, which in some cases is another viable option when expert data is available.\n",
    "\n",
    "\n",
    "In this exercise, we will collect two datasets: one with expert and another with suboptimal data. The goal of the agent will be to get as close as possible to the target.\n",
    "\n",
    "I - **expert policy**: collect ~ 1000 steps\n",
    "\n",
    "II  - **Suboptimal policy**:  collect ~ 2000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obstacle_selected = widget_list([ObstacleTypes.obstacle_8x8_wall_with_door])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = EnvFactory.Grid_2D_8x8_discrete\n",
    "\n",
    "# Grid configuration\n",
    "OBSTACLE = obstacle_selected.value\n",
    "INITIAL_STATE = (7, 7)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "env_2D_grid_initial_config = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE,\n",
    "    target_state=FINAL_STATE,\n",
    ")\n",
    "\n",
    "env = ENV.get_env(render_mode=render_mode, grid_config=env_2D_grid_initial_config)\n",
    "\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Behavior policies and datasets configurations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEHAVIOR_POLICY_I = BehaviorPolicy2dGridFactory.move_up\n",
    "DATA_SET_IDENTIFIER_I = \"_expert\"\n",
    "NUM_STEPS_I = 1000\n",
    "\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicy2dGridFactory.move_left\n",
    "DATA_SET_IDENTIFIER_II = \"_suboptimal\"\n",
    "NUM_STEPS_II = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_selected = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV,\n",
    "    render_mode=render_mode,\n",
    "    behavior_policy=policy_selected.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Collect data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from offline_rl.generate_custom_minari_datasets.generate_minari_dataset import (\n",
    "    create_combined_minari_dataset,\n",
    ")\n",
    "\n",
    "config_combined_data = create_combined_minari_dataset(\n",
    "    env_name=ENV,\n",
    "    dataset_identifiers=(DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "    num_collected_points=(NUM_STEPS_I, NUM_STEPS_II),\n",
    "    behavior_policies=(BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "    combined_dataset_identifier=\"combined_data_sets_offline_rl\",\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    ")\n",
    "\n",
    "dataset_availables = [\n",
    "    config_combined_data.data_set_name\n",
    "] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(buffer_data, env)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis\n",
    "\n",
    "Note that we have four peaks. The ones at (2,7) and (3,7) come from policy-I, which goes towards the target but stops before reaching it. The other two peaks at (6,0) and (7,0) are produced by policy-II, which drifts the agent to the left with noise. **It is important to notice that the amount of collected data at state (5,7) is very little, but this state is crucial if we want to approach the target.**\n",
    "\n",
    "What do you think a BC algorithm would do? What about an offline one?\n",
    "\n",
    "<div style=\"margin-top: 20px;\">\n",
    "    <div style=\"display: flex; justify-content: space-between;\">\n",
    "        <div style=\"width: 100%;\">\n",
    "            <img src=\"_static/images/nb_96_critical_state.png\" alt=\"Snow\" style=\"width:100%;\">\n",
    "        </div>\n",
    "        <div style=\"width: 100%;\">\n",
    "            <img src=\"_static/images/96_critical_action_states.png\" alt=\"KL divergence\" width=80%>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4-5: Select offline policies and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from offline_rl.offline_policies.policy_registry import RLPolicyFactory\n",
    "\n",
    "offline_rl_policies = [\n",
    "    RLPolicyFactory.bcq_discrete,\n",
    "    RLPolicyFactory.cql_discrete,\n",
    "    RLPolicyFactory.imitation_learning,\n",
    "]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0 * len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "# Metadata for the offline policy. Included the Minari metadata as well as the policy model configuration.\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    rl_policy_model=selected_offline_rl_policy.value,\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "# Hyperparameters to be used in the training.\n",
    "offline_training_hyperparams = OfflineTrainingHyperparams(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OfflineRLTraining.training(offline_training_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_obstacles = [ObstacleTypes.obstacle_8x8_wall_with_door]\n",
    "selected_obstacle = widget_list(available_obstacles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (7, 7)\n",
    "FINAL_STATE = (0, 7)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    rl_policy_model=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "policy = OfflineRLTraining.restore_policy(offline_policy_config)\n",
    "\n",
    "env.set_new_obstacle_map(selected_obstacle.value.value)\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "# snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False,\n",
    "    inline=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary and conclusions\n",
    "\n",
    "**BCQ and the CQL policies are able to learn the expert data**\n",
    "\n",
    "**Imitation learning cannot make it because it cannot learn the critical state-action pairs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise II "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now, we'll explore how BCQ and CQL, address the issue of connecting suboptimal trajectories in order to get new ones with higer rewards (stitching property). We will see how they compare with imitation learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start again with the previous setup. So, as we did before, we will create again two datasets: one from a policy moving suboptimal from (0,0) to (2,4), and the other from another policy moving from (4,0) to (7,7). The goal is to find an agent capable of connecting trajectories coming from both datasets, in order to find the optimal path between (2,0) and (2,4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 1: Create the environment\n",
    "\n",
    "**Create the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = EnvFactory.Grid_2D_8x8_discrete\n",
    "\n",
    "OBSTACLE = ObstacleTypes.obst_free_8x8\n",
    "INITIAL_STATE_POLICY_I = (0, 0)\n",
    "INITIAL_STATE_POLICY_II = (2, 0)\n",
    "FINAL_STATE_POLICY = (2, 4)\n",
    "\n",
    "\n",
    "env_2D_grid_initial_config_I = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_I,\n",
    "    target_state=FINAL_STATE_POLICY,\n",
    ")\n",
    "\n",
    "env_2D_grid_initial_config_II = Grid2DInitialConfig(\n",
    "    obstacles=OBSTACLE,\n",
    "    initial_state=INITIAL_STATE_POLICY_II,\n",
    "    target_state=FINAL_STATE_POLICY,\n",
    ")\n",
    "\n",
    "env = ENV.get_env(render_mode=render_mode, grid_config=env_2D_grid_initial_config_I)\n",
    "snapshot_env(env)\n",
    "\n",
    "env = ENV.get_env(render_mode=render_mode, grid_config=env_2D_grid_initial_config_II)\n",
    "snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 2: Create Minari datasets\n",
    "\n",
    "**Let's see how well offline RL algorithms can deal with the stitching property. We will examine some edge cases to compare them with some of the algorithms we have already studied before.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDENTIFIER_COMBINED_DATASETS = \"_stiching_property_I\"\n",
    "\n",
    "# Dataset I with 2000 collected points\n",
    "BEHAVIOR_POLICY_I = BehaviorPolicy2dGridFactory.deterministic_8x8\n",
    "DATA_SET_IDENTIFIER_I = \"_longer_path\"\n",
    "NUM_STEPS_I = 2000\n",
    "\n",
    "# Dataset II with 1000 points\n",
    "# BEHAVIOR_POLICY_II = BehaviorPolicyType.behavior_8x8_eps_greedy_4_0_to_7_7\n",
    "BEHAVIOR_POLICY_II = BehaviorPolicy2dGridFactory.move_right\n",
    "DATA_SET_IDENTIFIER_II = \"_short_path\"\n",
    "NUM_STEPS_II = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_policy_to_render = widget_list([BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_2D_grid_initial_config = (\n",
    "    env_2D_grid_initial_config_I\n",
    "    if select_policy_to_render.value == BEHAVIOR_POLICY_I\n",
    "    else env_2D_grid_initial_config_II\n",
    ")\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=ENV,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=select_policy_to_render.value,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Create datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_combined_data = create_combined_minari_dataset(\n",
    "    env_name=ENV,\n",
    "    dataset_identifiers=(DATA_SET_IDENTIFIER_I, DATA_SET_IDENTIFIER_II),\n",
    "    num_collected_points=(NUM_STEPS_I, NUM_STEPS_II),\n",
    "    behavior_policies=(BEHAVIOR_POLICY_I, BEHAVIOR_POLICY_II),\n",
    "    combined_dataset_identifier=\"combined_dataset\",\n",
    "    env_2d_grid_initial_config=(env_2D_grid_initial_config_I, env_2D_grid_initial_config_II),\n",
    ")\n",
    "buffer_data = load_buffer_minari(config_combined_data.data_set_name)\n",
    "data_size = len(buffer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_availables = [\n",
    "    config_combined_data.data_set_name\n",
    "] + config_combined_data.children_dataset_names\n",
    "selected_data_set = widget_list(dataset_availables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 3: Feed data into replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_data = load_buffer_minari(selected_data_set.value)\n",
    "len_buffer = len(buffer_data)\n",
    "\n",
    "# Compute state-action data distribution\n",
    "state_action_count_data, _ = get_state_action_data_and_policy_grid_distributions(\n",
    "    buffer_data, env, normalized=False\n",
    ")\n",
    "\n",
    "if \"start_0_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((0, 0))\n",
    "    snapshot_env(env)\n",
    "elif \"start_2_0\" in selected_data_set.value:\n",
    "    env.set_starting_point((2, 0))\n",
    "    snapshot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### STEP 4: Select offline policies and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_rl_policies = [RLPolicyFactory.bcq_discrete, RLPolicyFactory.cql_discrete]\n",
    "selected_offline_rl_policy = widget_list(offline_rl_policies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Offiline - Training\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "BATCH_SIZE = 128\n",
    "STEP_PER_EPOCH = 1.0 * len_buffer\n",
    "NUMBER_TEST_ENVS = 1\n",
    "\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    rl_policy_model=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "\n",
    "# Hyperparameters to be used in the training.\n",
    "offline_training_hyperparams = OfflineTrainingHyperparams(\n",
    "    offline_policy_config=offline_policy_config,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    number_test_envs=NUMBER_TEST_ENVS,\n",
    "    step_per_epoch=STEP_PER_EPOCH,\n",
    "    restore_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OfflineRLTraining.training(offline_training_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Restore and visualize trained policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVED_POLICY_NAME = \"policy_best_reward.pth\"\n",
    "SAVED_POLICY_NAME = \"policy.pth\"\n",
    "INITIAL_STATE = (0, 0)\n",
    "FINAL_STATE = (2, 4)\n",
    "\n",
    "offline_policy_config = TrainedPolicyConfig(\n",
    "    name_expert_data=selected_data_set.value,\n",
    "    rl_policy_model=selected_offline_rl_policy.value,\n",
    "    render_mode=render_mode,\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "policy = OfflineRLTraining.restore_policy(offline_policy_config)\n",
    "log_name = Path(selected_data_set.value) / Path(selected_offline_rl_policy.value)\n",
    "log_path = get_trained_policy_path() / log_name / SAVED_POLICY_NAME\n",
    "policy.load_state_dict(torch.load(str(log_path), map_location=\"cpu\"))\n",
    "\n",
    "env.set_starting_point(INITIAL_STATE)\n",
    "env.set_goal_point(FINAL_STATE)\n",
    "# snapshot_env(env)\n",
    "\n",
    "offpolicy_rendering(\n",
    "    env_or_env_name=env,\n",
    "    render_mode=render_mode,\n",
    "    policy_model=policy,\n",
    "    env_2d_grid_initial_config=env_2D_grid_initial_config,\n",
    "    num_frames=100,\n",
    "    imitation_policy_sampling=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Let's now change the dataset distribution. We'll collect 600 points with the first behavior policy and 100 with the second one. In this case, the probability of taking the suboptimal path will be higher. What paths are chosen by the algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Offline RL proves valuable in various scenarios, especially when:\n",
    "\n",
    "a. Robots require intelligent behavior in complex open-world environments demanding extensive training data due to robust visual perception requirements. (complex environment modeling and extensive data collection)\n",
    "\n",
    "b. Robot grasping tasks, which involve expert data that cannot be accurately simulated, providing an opportunity to assess our BCQ algorithm.\n",
    "\n",
    "c. Robotic navigation tasks, where offline RL aids in crafting effective navigation policies using real-world data.\n",
    "\n",
    "d. Autonomous driving, where ample expert data and an offline approach enhance safety.\n",
    "\n",
    "e. Healthcare applications, where safety is paramount due to the potential serious consequences of inaccurate forecasts.\n",
    "\n",
    "... and many more.\n",
    "\n",
    "However, if you have access to an environment with abundant data, online Reinforcement Learning (RL) can be a powerful choice due to its potential for exploration and real-time feedback. Nevertheless, the landscape of RL is evolving, and a data-centric approach is gaining prominence, exemplified by vast datasets like X-Embodiment. It's becoming evident that robots trained with diverse data across various scenarios tend to outperform those solely focused on specific tasks. Furthermore, leveraging multitask trained agents for transfer learning can be a valuable strategy for addressing your specific task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
